{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "First of all, we load the necessary pre-processing libraries such as:\n",
    "\n",
    "+ `pyvista`\n",
    "+ `xarray`\n",
    "+ `jigsaw`\n",
    "+ `meshio`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import meshio\n",
    "import meshplex\n",
    "import jigsawpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from time import process_time\n",
    "from gospl._fortran import definegtin\n",
    "\n",
    "import matplotlib\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_size = 7\n",
    "matplotlib.rcParams['xtick.labelsize'] = label_size\n",
    "matplotlib.rcParams['ytick.labelsize'] = label_size\n",
    "matplotlib.rc('font', size=6)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured global mesh construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a folder where input files will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution of the region of interest in km\n",
    "res_mesh = 25\n",
    "\n",
    "input_path = \"input\"+str(res_mesh)+\"km\" \n",
    "if not os.path.exists(input_path):\n",
    "    os.makedirs(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example focus on the Gulf of Mexico and we specify the extent of the region of interest. \n",
    "\n",
    ":::{note}\n",
    "\n",
    "We define a region which encapsulate the large basins which drains into the Gulf.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitudinal extent\n",
    "gom_lon = [-135, -45]\n",
    "\n",
    "# Latitudinal extent\n",
    "gom_lat = [10, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Jigsaw` processing folder\n",
    "\n",
    "The `Jigsaw` library requires an initial mesh. In our case we work with global dataset and we define an `ellipsoid-grid` with a 1 degree resolution containing a single `variable` set to:\n",
    "\n",
    "+ -100 outside the region of interest, \n",
    "+ 100 in the specified region and \n",
    "+ 0 on the border of the region (5 degree width).  \n",
    "\n",
    "This grid is built with the `buildRegMesh` function defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRegMesh(region_lon, region_lat, outfile, glon=181, glat=91):\n",
    "    '''\n",
    "    Create a lon/lat regular grid with values set to -100 everywhere except\n",
    "    within the specified region where it is set to 100 and within 5 degree\n",
    "    surrounding the region where it is set to 0.\n",
    "    \n",
    "    :arg region_lon: longitudinal extent \n",
    "    :arg region_lat: latitudinal extent  \n",
    "    :arg outfile: name of the output file\n",
    "    :arg glon: number of points along the longitudes (default: 181)\n",
    "    :arg glat: number of points along the latitudes (default: 91)\n",
    "    \n",
    "    '''\n",
    "    lon = np.linspace(-180.0, 180, glon)\n",
    "    lat = np.linspace(-90.0, 90, glat)\n",
    "    zval = np.zeros((glon, glat))\n",
    "    ds = xr.Dataset(\n",
    "                {\"z\": ([\"longitude\", \"latitude\"], zval),},\n",
    "                coords={\n",
    "                    \"longitude\": ([\"longitude\"], lon),\n",
    "                    \"latitude\": ([\"latitude\"], lat),\n",
    "                },\n",
    "                )\n",
    "    \n",
    "    lo = ds.coords[\"longitude\"]\n",
    "    la = ds.coords[\"latitude\"]\n",
    "    ds[\"flag\"] = xr.full_like(ds.z, fill_value=-100)\n",
    "    ds[\"flag\"].loc[dict(longitude=lo[(lo > region_lon[0]-5.) \n",
    "                                     & (lo < region_lon[1]+5.)], \n",
    "                     latitude=la[(la > region_lat[0]-5.) \n",
    "                                 & (la < region_lat[1]+5.)])] = 0\n",
    "    \n",
    "    \n",
    "    ds[\"flag\"].loc[dict(longitude=lo[(lo > region_lon[0]) \n",
    "                                     & (lo < region_lon[1])], \n",
    "                     latitude=la[(la > region_lat[0]) \n",
    "                                 & (la < region_lat[1])])] = 100\n",
    "    \n",
    "    fval = ds[\"flag\"].values.flatten()\n",
    "    \n",
    "    f = open(outfile, \"w+\")\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(\"mshid=3;ellipsoid-grid\\n\")\n",
    "        f.write(\"mdims=2\\n\")\n",
    "\n",
    "        f.write(\"coord=1;%d\\n\" % (len(lon)))\n",
    "        f.write(\"\\n\".join(map(str, lon)))\n",
    "        \n",
    "        f.write(\"\\ncoord=2;%d\\n\" % (len(lat)))\n",
    "        f.write(\"\\n\".join(map(str, lat)))\n",
    "        \n",
    "        f.write(\"\\nvalue=%d;1\\n\" % (len(fval)))\n",
    "        f.write(\"\\n\".join(map(str, fval)))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regfile = os.path.join(input_path, \"topo.msh\")\n",
    "ds = buildRegMesh(gom_lon, gom_lat, regfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the unstructured grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use `jigsaw` to generate the unstructured mesh from our regular grid...\n",
    "\n",
    "To do so we define a new function `unstructuredMesh` which takes 3 arguments. The first 2 specify the created `jigsaw` regular file name and directory. The last one `hfn` defines the space conditions for the jigsaw algorithm and consists of 3 values:\n",
    "\n",
    "+ the spacing in km for the unstructured mesh where the regular grid input defined previously is equal to -100,\n",
    "+ the spacing in km for the unstructured mesh where the regular grid input defined previously is equal to 0, + the spacing in km for region set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstructuredMesh(dst_path, regfile, hfn):\n",
    "    '''\n",
    "    Unstructured mesh generated with `jigsaw` from a regular grid defined with values set to -100,0 and 100.\n",
    "    \n",
    "    :arg dst_path: name of the folder containing the `jigsaw` files same as the regular grid one\n",
    "    :arg regfile: name of the regular grid defined in the previous function\n",
    "    :arg hfn: space conditions for the jigsaw algorithm\n",
    "    \n",
    "    '''\n",
    "\n",
    "    meshfile = os.path.join(dst_path, \"mesh.msh\")\n",
    "    spacefile = os.path.join(dst_path, \"spac.msh\")\n",
    "    outfile = os.path.join(dst_path, \"mesh.vtk\")\n",
    "\n",
    "    t0 = process_time()\n",
    "    opts = jigsawpy.jigsaw_jig_t()\n",
    "    topo = jigsawpy.jigsaw_msh_t()\n",
    "    geom = jigsawpy.jigsaw_msh_t()\n",
    "    mesh = jigsawpy.jigsaw_msh_t()\n",
    "    hmat = jigsawpy.jigsaw_msh_t()\n",
    "\n",
    "    jigsawpy.loadmsh(regfile, topo)\n",
    "    print(\"Load topography grid (%0.02f seconds)\" % (process_time() - t0))\n",
    "\n",
    "    t0 = process_time()\n",
    "    opts.geom_file = os.path.join(dst_path, \"topology.msh\")\n",
    "    opts.jcfg_file = os.path.join(dst_path, \"config.jig\")\n",
    "    opts.mesh_file = meshfile\n",
    "    opts.hfun_file = spacefile\n",
    "\n",
    "    geom.mshID = \"ellipsoid-mesh\"\n",
    "    geom.radii = np.full(3, 6.371e003, dtype=geom.REALS_t)\n",
    "    jigsawpy.savemsh(opts.geom_file, geom)\n",
    "\n",
    "    hmat.mshID = \"ellipsoid-grid\"\n",
    "    hmat.radii = geom.radii\n",
    "    hmat.xgrid = topo.xgrid * np.pi / 180.0\n",
    "    hmat.ygrid = topo.ygrid * np.pi / 180.0\n",
    "\n",
    "    # Set HFUN gradient-limiter\n",
    "    hmat.value = np.full(topo.value.shape, hfn[0], dtype=hmat.REALS_t)\n",
    "    hmat.value[topo.value == 0] = hfn[1]\n",
    "    hmat.value[topo.value > 10] = hfn[2]\n",
    "\n",
    "    hmat.slope = np.full(topo.value.shape, +0.050, dtype=hmat.REALS_t)\n",
    "    jigsawpy.savemsh(opts.hfun_file, hmat)\n",
    "    jigsawpy.cmd.marche(opts, hmat)\n",
    "    print(\"Build space function (%0.02f seconds)\" % (process_time() - t0))\n",
    "\n",
    "    t0 = process_time()\n",
    "    opts.hfun_scal = \"absolute\"\n",
    "    opts.hfun_hmax = float(\"inf\")  # null HFUN limits\n",
    "    opts.hfun_hmin = float(+0.00)\n",
    "\n",
    "    opts.mesh_dims = +2  # 2-dim. simplexes\n",
    "\n",
    "    opts.optm_qlim = +9.5e-01  # tighter opt. tol\n",
    "    opts.optm_iter = +32\n",
    "    opts.optm_qtol = +1.0e-05\n",
    "\n",
    "    jigsawpy.cmd.tetris(opts, 3, mesh)\n",
    "    print(\"Perform triangulation (%0.02f seconds)\" % (process_time() - t0))\n",
    "\n",
    "    t0 = process_time()\n",
    "    apos = jigsawpy.R3toS2(geom.radii, mesh.point[\"coord\"][:])\n",
    "\n",
    "    apos = apos * 180.0 / np.pi\n",
    "\n",
    "    zfun = interpolate.RectBivariateSpline(topo.ygrid, topo.xgrid, topo.value)\n",
    "\n",
    "    mesh.value = zfun(apos[:, 1], apos[:, 0], grid=False)\n",
    "\n",
    "    jigsawpy.savevtk(outfile, mesh)\n",
    "    jigsawpy.savemsh(opts.mesh_file, mesh)\n",
    "    print(\"Get unstructured mesh (%0.02f seconds)\" % (process_time() - t0))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the resolution for our unstructured mesh. \n",
    "\n",
    "Here, we will chose a coarse resolution of 150000 km outside of the region of interest, 100 km around the region and 25 km within the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfn = np.zeros(3)\n",
    "hfn[0] = 150000.\n",
    "hfn[1] = 100.\n",
    "hfn[2] = float(res_mesh)\n",
    "\n",
    "print('Chosen resolution in km',hfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call `jigsaw` meshing function... \n",
    "\n",
    ":::{caution}\n",
    "\n",
    "This function might takes some times if you choose a fine resolution (<10 km) so be careful!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstructuredMesh(input_path, regfile, hfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the mesh properties in our jupyter notebook:\n",
    "\n",
    "+ coordinates of each vertice\n",
    "+ cells defining the connectivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfile = os.path.join(input_path, \"mesh.vtk\")\n",
    "\n",
    "umesh = meshio.read(meshfile)\n",
    "\n",
    "coords = umesh.points\n",
    "coords = (coords / 6.371e003) * 6378137.0\n",
    "cells = umesh.cells_dict[\"triangle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the mesh. After running the cell below you will be able to use the top left widget on the graph to select the `surface with edges` as shown below.\n",
    "\n",
    "\n",
    "```{figure} images/selectedges.png\n",
    ":height: 100px\n",
    ":name: figure-example\n",
    "\n",
    "`pyvista` selecting edges\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfile = os.path.join(input_path, \"mesh.vtk\")\n",
    "mesh = pv.read(meshfile)\n",
    "\n",
    "plotter = pv.PlotterITK()\n",
    "plotter.add_mesh(mesh, scalars=\"value\")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping elevations \n",
    "\n",
    "Now that we have our unstructured mesh, we will interpolate the required variables on the new mesh. \n",
    "We will start with the elevation. \n",
    "\n",
    ":::{note}\n",
    "\n",
    "As most of the available dataset are defined in lon/lat coordinates, we first define a function `xyz2lonlat` to perform the conversion between cartesian points of the spherical triangulation to lat/lon.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyz2lonlat(coords, radius=6378137.0):\n",
    "    \"\"\"\n",
    "    Convert x,y,z representation of cartesian points of the\n",
    "    spherical triangulation to lat/lon.\n",
    "    \"\"\"\n",
    "\n",
    "    gLonLat = np.zeros((len(coords), 2))\n",
    "\n",
    "    gLonLat[:, 1] = np.arcsin(coords[:, 2] / radius)\n",
    "    gLonLat[:, 0] = np.arctan2(coords[:, 1], coords[:, 0])\n",
    "    gLonLat[:, 1] = np.mod(np.degrees(gLonLat[:, 1]) + 90, 180.0)\n",
    "    gLonLat[:, 0] = np.mod(np.degrees(gLonLat[:, 0]) + 180.0, 360.0)\n",
    "\n",
    "    return gLonLat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat = xyz2lonlat(coords, radius=6378137.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we chose to use the ETOPO5 topography file, obviously other elevation grids could be used. \n",
    "\n",
    "The `netcdf` file for this specific dataset can be accessed via `THREDDS` protocol by specifying the related `url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset files\n",
    "\n",
    "# etopo2\n",
    "#infile = 'https://data.pmel.noaa.gov/pmel/thredds/dodsC/data/PMEL/smith_sandwell_topo_v8_2.nc'\n",
    "\n",
    "# etopo5 url\n",
    "infile = 'http://ferret.pmel.noaa.gov/pmel/thredds/dodsC/data/PMEL/etopo5.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation with `xarray`\n",
    "\n",
    "We will use `xarray` library to open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read topo\n",
    "data = xr.open_dataset(infile)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first regrid the longitude (`ETOPO05_X`) to match with our `jigsaw` unstructured mesh (*i.e* between -180 and 180 instead of 0 to 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.coords['ETOPO05_X'] = (data.coords['ETOPO05_X'] + 180) % 360 - 180\n",
    "data = data.sortby(data.ETOPO05_X)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now only take into account the elevation within our specified region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a new variable newz\n",
    "data = data.assign(newz=data[\"ROSE\"])\n",
    "\n",
    "# We mask all values not in the specified region\n",
    "mask = ((data.coords[\"ETOPO05_Y\"] > gom_lat[0]) & (data.coords[\"ETOPO05_Y\"] < gom_lat[1])\n",
    "        & (data.coords[\"ETOPO05_X\"] > gom_lon[0]) & (data.coords[\"ETOPO05_X\"] < gom_lon[1]))\n",
    "\n",
    "# We set all values not in the specified region with an elevation of -10000.\n",
    "data[\"newz\"] = xr.where(np.logical_not(mask), -10000, data[\"newz\"])\n",
    "dataArray = data[\"newz\"].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate on the unstructured mesh\n",
    "\n",
    "Once the elevation array has been built, we now interpolate from the regular dataset to the unstructured mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply a smoothing on the dataset if needed...\n",
    "dataArray = ndimage.gaussian_filter(dataArray, sigma=0.1)\n",
    "\n",
    "# Map regular mesh lon/lat \n",
    "ilons = dataArray.shape[0] * lonlat[:, 0] / float(dataArray.shape[0])\n",
    "ilats = dataArray.shape[1] * lonlat[:, 1] / float(dataArray.shape[1])\n",
    "\n",
    "# Create the regular grid coordinates and store it as rcoords\n",
    "icoords = np.stack((ilons, ilats))\n",
    "rlons = icoords[0, :] * dataArray.shape[0] / 360.0\n",
    "rlats = icoords[1, :] * dataArray.shape[1] / 180.0\n",
    "\n",
    "rcoords = np.zeros(icoords.shape)\n",
    "rcoords[0, :] = rlons\n",
    "rcoords[1, :] = rlats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate the elevations on the global unstructured mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshz = ndimage.map_coordinates(dataArray, rcoords, order=2, \n",
    "                                mode=\"nearest\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save elevation grid on disk\n",
    "\n",
    "We now build `gospl` unstructured mesh input for the elevation data which needs to be a `Numpy` compressed file containing:\n",
    "\n",
    "+ mesh coordinates, \n",
    "+ cells, \n",
    "+ each vertice neighbours indices and \n",
    "+ the nodes elevation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set meshplex triangular mesh\n",
    "Gmesh = meshplex.MeshTri(coords, cells)\n",
    "s = Gmesh.idx_hierarchy.shape\n",
    "a = np.sort(Gmesh.idx_hierarchy.reshape(s[0], -1).T)\n",
    "Gmesh.edges = {\"points\": np.unique(a, axis=0)}\n",
    "\n",
    "# Get each vertice neighbours indices\n",
    "ngbNbs, ngbID = definegtin(len(coords), Gmesh.cells(\"points\"), \n",
    "                           Gmesh.edges[\"points\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the file in an input folder for `gospl` run (here called `gospl_data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gospl input files directory\n",
    "mesh_path = \"gospl_data\" \n",
    "if not os.path.exists(mesh_path):\n",
    "    os.makedirs(mesh_path)\n",
    "\n",
    "# Set elevation file name\n",
    "elevfname = os.path.join(mesh_path, \"mesh\"+str(res_mesh)+\"km\")\n",
    "\n",
    "\n",
    "# Save the mesh as compressed numpy file for gospl\n",
    "np.savez_compressed(elevfname, v=coords, c=cells, n=ngbID[:, :8].astype(int), \n",
    "                    z=meshz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping rainfall\n",
    "\n",
    "We will now load a precipitation grid and interpolate the dataset on the unstructured mesh.\n",
    "\n",
    "Here we chose the CPC collection of precipitation data sets (2.5°x2.5°) containing global monthly values since 1979.\n",
    "\n",
    "````{margin}\n",
    "```{seealso}\n",
    "[PSL](https://psl.noaa.gov/data/gridded/help.html#opendap) climate research data resources is useful for finding precipitation map as well as the THREDDS [catalog](https://psl.noaa.gov/thredds/catalog/Datasets/catalog.html).\n",
    "```\n",
    "````\n",
    "\n",
    "The `netcdf` file for our dataset can be accessed via `THREDDS` protocol by specifying the related `url`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset file url\n",
    "ncfile = 'https://psl.noaa.gov/thredds/dodsC/Datasets/cmap/enh/precip.mon.mean.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation with `xarray`\n",
    "\n",
    "Here again we will use `xarray` to open the file and perform several changes on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read rain\n",
    "dr = xr.open_dataset(ncfile)\n",
    "dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of average monthly rate of precipitation. `gospl` requires rainfall in `m/yr` we therefore use the `groupby` function to compute the yearly mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute annual mean\n",
    "dryears = dr.groupby('time.year').mean('time')\n",
    "dryears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the annual mean for the entire dataset running from 1979 to 2021:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dryearly = dryears.mean('year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now interpolate the dataset on a higher resolution grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new longitudes for interpolation\n",
    "new_lon = np.linspace(dryearly.lon[0], dryearly.lon[-1], \n",
    "                      dryearly.dims[\"lon\"] * 4)\n",
    "\n",
    "# Set new latitudes for interpolation\n",
    "new_lat = np.linspace(dryearly.lat[0], dryearly.lat[-1], \n",
    "                      dryearly.dims[\"lat\"] * 4)\n",
    "\n",
    "# Interpolation\n",
    "drain = dryearly.interp(lat=new_lat, lon=new_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the elevation dataset, we will now regrid the longitude to match with the `jigsaw` unstructured mesh (*i.e* between -180 and 180 instead of 0 to 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drain.coords['lon'] = (drain.coords['lon'] + 180) % 360 - 180\n",
    "drain = drain.sortby(drain.lon).fillna(0)\n",
    "drain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now only consider the dataset within our specified region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate\n",
    "drain_itp = drain.interp(lon=data['ETOPO05_X'].values, lat=data['ETOPO05_Y'].values)\n",
    "drain_itp = drain_itp.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a new variable rain\n",
    "drain_itp = drain_itp.assign(rain=drain_itp[\"precip\"])\n",
    "\n",
    "# We mask all values not in the specified region\n",
    "mask = ((drain_itp.coords[\"lat\"] > gom_lat[0]) & (drain_itp.coords[\"lat\"] < gom_lat[1])\n",
    "        & (drain_itp.coords[\"lon\"] > gom_lon[0]) & (drain_itp.coords[\"lon\"] < gom_lon[1]))\n",
    "\n",
    "drain_itp[\"rain\"] = xr.where(np.logical_not(mask), 0, drain_itp[\"rain\"])\n",
    "drain_itp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate on the unstructured mesh\n",
    "\n",
    "Once the rainfall array has been built, we now interpolate from the regular dataset to the unstructured mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buld the rain array convertion from mm/day to m/year\n",
    "rainArray = drain_itp.rain.values.T*366./1000.\n",
    "rainArray = np.nan_to_num(rainArray)\n",
    "rainArray[rainArray<0.] = 0\n",
    "    \n",
    "# We can apply a smoothing on the dataset if needed...\n",
    "rainArray = ndimage.gaussian_filter(rainArray, sigma=2)\n",
    "rainArray[rainArray<0.] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate the rainfalls on the global unstructured mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the paleogrid on global mesh\n",
    "meshr = ndimage.map_coordinates(rainArray, rcoords, order=2,\n",
    "                                mode=\"nearest\").astype(float)\n",
    "\n",
    "meshr[meshr<0.] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write precipitation grid on disk\n",
    "\n",
    "We now build `gospl` precipitation input data as a `Numpy` compressed file with the precipitation values for each mesh vertice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set rainfall file name\n",
    "npzrain = os.path.join(mesh_path, \"rain\"+str(res_mesh)+\"km\")\n",
    "\n",
    "# Save the rainfall as compressed numpy file for gospl\n",
    "np.savez_compressed(npzrain, r=meshr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical tectonic forcing\n",
    "\n",
    "To define tectonic conditions (uplift/subsidence), we can chose to import existing dataset. Here however we will be using a much simpler approach where we define: \n",
    "\n",
    "+ uplift rate of 0.1 cm/yr in regions above 500 m, \n",
    "+ no vertical movement between 500 and -10 m, and \n",
    "+ subsidence rate of -0.1 cm/yr for elevation below -10 m. \n",
    "\n",
    ":::{note}\n",
    "\n",
    "The tectonic forcing conditions in `gospl` is set by a sequence of events defined by a starting time (start) and either a vertical only forcing (*e.g.* uplift and/or subsidence defined with `mapV`) or a fully 3D displacement mesh `mapH`. These displacements are set in **metres per year**. In our case we will only have one event during the 50 thousand years.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tecto = np.zeros(meshz.shape)\n",
    "\n",
    "# Defining the tectonic rates based on elevation...\n",
    "tecto[meshz>500] = 0.1/100.\n",
    "tecto[meshz<-10] = -0.1/100.\n",
    "\n",
    "# Set tectonic file name\n",
    "npztecto = os.path.join(mesh_path, \"tecto\"+str(res_mesh)+\"km\")\n",
    "\n",
    "# Save the tectonic as compressed numpy file for gospl\n",
    "np.savez_compressed(npztecto, z=tecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save inputs as a `vtk` file\n",
    "\n",
    "Before going further, you can check the mesh and interpolated dataset by building a `VTK` file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paleovtk = elevfname + \".vtk\"\n",
    "\n",
    "# Define mesh\n",
    "vis_mesh = meshio.Mesh(coords, {\"triangle\": cells}, \n",
    "                       point_data={\"elev\": meshz, \"precip\": meshr, \"tecto\": tecto})\n",
    "\n",
    "# Write it disk\n",
    "meshio.write(paleovtk, vis_mesh)\n",
    "\n",
    "print(\"Writing VTK input file as {}\".format(paleovtk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s visualise the mesh with `pyvista` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = pv.read(paleovtk)\n",
    "elev = mesh.get_array(name='elev')\n",
    "\n",
    "earthRadius = 6.371e6\n",
    "scale = 10.\n",
    "factor = 1.+ (elev/earthRadius)*scale\n",
    "\n",
    "mesh.points[:, 0] *= factor\n",
    "mesh.points[:, 1] *= factor\n",
    "mesh.points[:, 2] *= factor\n",
    "\n",
    "contour = mesh.contour([0])\n",
    "\n",
    "plotter = pv.PlotterITK()\n",
    "plotter.add_mesh(mesh, scalars=\"elev\")\n",
    "plotter.add_mesh(contour, color=\"black\", opacity=1.)\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea level curve\n",
    "\n",
    "We will impose a sea-level increase over the simulated period (50 thousand years). To do so we need to write a file containing 2 columns (time and sea-level position). Assuming a continuous increase at 0.1 cm/yr:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation start time\n",
    "tstart = 0.\n",
    "\n",
    "# Simulation end time\n",
    "tend = 50000.\n",
    "\n",
    "# Starting sea-level position\n",
    "sea0 = -25.\n",
    "\n",
    "# Time step for sea-level\n",
    "sea_dt = 10000.\n",
    "\n",
    "# sea level rise in m/yr\n",
    "rate = 0.001\n",
    "\n",
    "# Define time\n",
    "times = np.arange(tstart, tend+sea_dt, sea_dt)\n",
    "\n",
    "# Get sea-level position\n",
    "sea_level = sea0 + times*rate\n",
    "\n",
    "# Create a pandas dataframe \n",
    "df = pd.DataFrame({'time':times,'sea':sea_level})\n",
    "\n",
    "# Save it to file\n",
    "seafile = os.path.join(mesh_path, \"sealevel.csv\")\n",
    "df.to_csv(seafile, columns=['time', 'sea'], sep=',', index=False ,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
